{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Callysto.ca Banner](https://github.com/callysto/curriculum-notebooks/blob/master/callysto-notebook-banner-top.jpg?raw=true)\n",
    "\n",
    "<a href=\"https://hub.callysto.ca/jupyter/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fcallysto%2Finteresting-problems&branch=main&subPath=notebooks/analysing-text-statistics.ipynb&depth=1\" target=\"_parent\"><img src=\"https://raw.githubusercontent.com/callysto/curriculum-notebooks/master/open-in-callysto-button.svg?sanitize=true\" width=\"123\" height=\"24\" alt=\"Open in Callysto\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing Text Statistics\n",
    "\n",
    "Let's try out some statistical analysis of text, including [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing), using a [public domain](https://en.wikipedia.org/wiki/Public_domain) book from [Project Gutenberg](http://www.gutenberg.org).\n",
    "\n",
    "The example we'll use is the [most downloaded](http://www.gutenberg.org/browse/scores/top) book, [Pride and Prejudice by Jane Austen](http://www.gutenberg.org/ebooks/1342). Running this first code cell will import and display the contents of the book.\n",
    "\n",
    "Feel free to change the link in the following code cell if you'd like to explore another book, but make sure you are using the `Plain Text UTF-8` link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_text_link = 'https://www.gutenberg.org/cache/epub/42671/pg42671.txt'\n",
    "\n",
    "import requests\n",
    "r = requests.get(gutenberg_text_link) # get the online book file\n",
    "r.encoding = 'utf-8' # specify the type of text encoding in the file\n",
    "text = r.text.split('***')[2] # get the part after the header\n",
    "text = text.replace(\"’\",\"'\").replace(\"“\",'\"').replace(\"”\",'\"') # replace any 'smart quotes'\n",
    "book_title = r.text[r.text.index('Title:')+7:r.text.index('Author:')-4] # find the book title\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a DataFrame of Chapters\n",
    "\n",
    "Now that we have the text of the book, let's split it into chapters. We'll use the Python [library](https://en.wikipedia.org/wiki/Library_(computing)) called [pandas](https://pandas.pydata.org) to create a [dataframe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) that includes the text and length of each chapter (number of characters, including spaces and punctuation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ch = [] # create an empty list that we will append to\n",
    "for chapter in text.split('CHAPTER'):\n",
    "    if len(chapter)>500: # so that we are getting actual book chapters\n",
    "        chapter = chapter.replace('\\r','').replace('\\n',' ') # replace the 'new line' characters with spaces\n",
    "        ch.append(chapter) # append the chapter text to the list\n",
    "book = pd.DataFrame(ch, columns=['Chapter Text']) # create a dataframe from the list\n",
    "book['Length'] = book['Chapter Text'].apply(len) # add a column with the length of each chapter\n",
    "book"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 0 is the introduction and transcriber's note, so we'll eliminate that, and create a column called `Chapter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = book.drop(0).reset_index().rename(columns={'index':'Chapter'})\n",
    "book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Chapter Lengths\n",
    "\n",
    "From that dataframe we can create a bar graph of chapter lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "px.bar(book, x='Chapter', y='Length', title='Characters per Chapter in '+book_title).update_xaxes(title='Chapter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Words by Type\n",
    "\n",
    "We'll use the [spaCy](https://spacy.io) natural language processing library to identify the [parts of speech](https://spacy.io/api/annotation#pos-tagging) in the text. For this example we'll just look at adjectives, verbs, nouns, and proper nouns, but you can add to the list on the first line in the code cell.\n",
    "\n",
    "This will take a while to run, and will result in a dataframe containing the number and percent of each of those parts of speech in each chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following two installation lines if you get errors\n",
    "#!pip install spacy --user\n",
    "#!python -m spacy download en_core_web_sm\n",
    "\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load() # set up natural language processing\n",
    "\n",
    "from collections import Counter\n",
    "def getWords(text, word_type):\n",
    "    words = []\n",
    "    for token in nlp(text):\n",
    "        if token.pos_ == word_type:\n",
    "            words.append(token.lemma_.strip().lower())\n",
    "    return Counter(words)\n",
    "\n",
    "#sum(book.iloc[0]['Verbs'].values())  # count the number of verbs in a chapter\n",
    "\n",
    "def getPartsOfSpeech(chapter, word_types):\n",
    "    parts_of_speech_list = [] # create an empty list\n",
    "    word_count = 0\n",
    "    for token in nlp(chapter): # loop through each token in the chapter\n",
    "        word_count += 1 # increment the word counter\n",
    "        part_of_speech = token.pos_\n",
    "        if part_of_speech in word_types: # if it is in the list of types we made\n",
    "            parts_of_speech_list.append(part_of_speech)\n",
    "    word_type_count = {} # create an empty dictionary\n",
    "    word_type_count['Word Count'] = word_count\n",
    "    for word_type in word_types:\n",
    "        word_type_count[word_type] = parts_of_speech_list.count(word_type) # add to the dictionary\n",
    "    return word_type_count\n",
    "\n",
    "word_types = ['ADJ', 'VERB', 'NOUN', 'PROPN'] # https://spacy.io/api/annotation#pos-tagging\n",
    "#getPartsOfSpeech(chapters['Chapter Text'][1], word_types)\n",
    "book[['Word Count'] + word_types] = pd.DataFrame(book['Chapter Text'].apply(getPartsOfSpeech, args=[word_types]).tolist())\n",
    "book"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the proportional occurances of those parts of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book['Adjectives %'] = book['ADJ']/book['Word Count']*100\n",
    "book['Verbs %'] = book['VERB']/book['Word Count']*100\n",
    "book['Nouns %'] = book['NOUN']/book['Word Count']*100\n",
    "book['Proper Nouns %'] = book['PROPN']/book['Word Count']*100\n",
    "book"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the proportional occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(book, y=['Adjectives %', 'Verbs %', 'Nouns %', 'Proper Nouns %'], title='Proportion of Word Types in '+book_title).update_xaxes(title='Chapter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Common Verbs\n",
    "\n",
    "To get an idea of the most common words in the text we can look at a part of speech, verbs for example, and count which are the most frequent.\n",
    "\n",
    "This will also take some time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def getWords(text, word_type):\n",
    "    words = []\n",
    "    for token in nlp(text):\n",
    "        if token.pos_ == word_type:\n",
    "            words.append(token.lemma_.strip().lower())\n",
    "    return Counter(words)\n",
    "\n",
    "#verbs = getWords(book['Chapter Text'].sum(), 'VERB')\n",
    "#print('There are',len(verbs),'unique verbs in the book.')\n",
    "book['Verbs'] = book['Chapter Text'].apply(getWords, args=['VERB'])\n",
    "\n",
    "verbs = Counter()\n",
    "for counts in book['Verbs']:\n",
    "    verbs.update(counts)\n",
    "print('There are',len(verbs),'unique verbs in the book.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(book.iloc[0]['Verbs'].values())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the `10` most common verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also choose a verb and plot its frequency by chapter as a percent of the total number of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'have'\n",
    "\n",
    "words_df['%'] = words_df[word]/parts_of_speech_df['Words']*100 # calculate the percent of the words in each chapter\n",
    "px.bar(words_df, y='%', title='Percent Frequency of the Word \"'+word+'\" by Chapter in '+book_title).update_xaxes(title='Chapter').update_yaxes(title='Percent Freqency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Common Names\n",
    "\n",
    "We can also look at character names and how often they occur in each chapter. The spaCy library does a fairly good job of identifying names, but you may see some false positives (words that aren't actually character names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_df = pd.DataFrame()\n",
    "for i in range(1,len(chapters)+1):\n",
    "    names_list = []\n",
    "    for token in nlp(chapters['Chapter Text'][i]):\n",
    "        #if token.pos_ == 'PROPN':\n",
    "        if token.ent_type_ == 'PERSON': # seems to be more reliable than part_of_speech == proper_noun\n",
    "            names_list.append(token.text)\n",
    "    names_df = names_df.append(Counter(names_list), ignore_index=True)\n",
    "names_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Character Names\n",
    "\n",
    "We can check out the list of words identified as names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in names_df.columns:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data\n",
    "\n",
    "If you'd like to remove columns that may categorized incorrectly or are uncommon, we can drop columns with fewer than five occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in names_df.columns:\n",
    "    if names_df[column].sum() < 5: # if there are fewer than five occurences\n",
    "        names_df.drop(columns=column, inplace=True)\n",
    "names_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Name Frequencies\n",
    "\n",
    "Let's make a bar graph of the top `20` most frequently mentioned characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = names_df.sum().sort_values(ascending=False).head(20)\n",
    "px.bar(d, title='Character Name Frequencies in '+book_title).update_yaxes(title='Frequency').update_xaxes(title='Name').update(layout_showlegend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative Name Frequencies over Time\n",
    "\n",
    "Since we have the text divided into chapters, let's visualize the cumulative mentions of the top `3` character names throughout the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how_many_names = 3\n",
    "main_character_names = names_df.sum().sort_values(ascending=False).head(how_many_names).index # list the top three\n",
    "main_characters = names_df[main_character_names].fillna(value=0) # create a new dataframe for top three\n",
    "main_characters.index = main_characters.index+1\n",
    "title = 'Cumulative Character Mentions of Time in '+book_title\n",
    "px.line(main_characters.cumsum(), title=title).update_yaxes(title='Total Mentions').update_xaxes(title='Chapter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "[Sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) is categorizing text based on its tone (negative, neutral, or positive).\n",
    "\n",
    "For this we will use the [vaderSentiment](https://github.com/cjhutto/vaderSentiment) library, then visualize the positive and negative sentiment by chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install vaderSentiment --user\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "sentiment_df = pd.DataFrame()\n",
    "for i in range(1,len(chapters)+1):\n",
    "    senti = analyzer.polarity_scores(chapters['Chapter Text'][i]) # analyze the sentiment of chapter\n",
    "    sentiment_df = sentiment_df.append(senti, ignore_index=True) # add to dataframe\n",
    "sentiment_df['neg'] = -sentiment_df['neg'] # change the sign of the negative sentiment\n",
    "sentiment_df.index = sentiment_df.index+1\n",
    "px.bar(sentiment_df, y=['pos', 'neg'], title=book_title+' Sentiment Analysis by Chapter').update(layout_showlegend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readability\n",
    "\n",
    "One last library to introduce, [textstat](https://github.com/shivam5992/textstat) for checking the readability, complexity, and grade level of text. It includes a [number of functions](https://github.com/shivam5992/textstat#list-of-functions), but we'll only use a few of them. This will take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --user textstat\n",
    "import textstat\n",
    "textstats = pd.DataFrame()\n",
    "for i in range(1,len(chapters)+1):\n",
    "    text = chapters['Chapter Text'][i]\n",
    "    textstats_data = {'Flesch-Kincaid Grade':textstat.flesch_kincaid_grade(text),\n",
    "                        'Gunning Fog Index':textstat.gunning_fog(text),\n",
    "                        'Linsear Write Formula':textstat.linsear_write_formula(text),\n",
    "                        'Readability':textstat.text_standard(text, float_output=True)}\n",
    "    textstats = textstats.append(textstats_data, ignore_index=True)\n",
    "textstats.index = textstats.index+1\n",
    "textstats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a dataframe of readability information, we can plot and describe the readability statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(textstats, y='Readability', title=book_title+' Readability by Chapter').update_xaxes(title='Chapter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textstats.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Hopefully this was an interesting introduction to text statistics such as chapter length, word frequency and word type frequency, sentiment analysis, and reading levels. You can, of course, use this analyse the text from any other online document in a similar way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Callysto.ca License](https://github.com/callysto/curriculum-notebooks/blob/master/callysto-notebook-banner-bottom.jpg?raw=true)](https://github.com/callysto/curriculum-notebooks/blob/master/LICENSE.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
