{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Callysto.ca Banner](https://github.com/callysto/curriculum-notebooks/blob/master/callysto-notebook-banner-top.jpg?raw=true)\n",
    "\n",
    "<a href=\"https://hub.callysto.ca/jupyter/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fcallysto%2Finteresting-problems&branch=main&subPath=notebooks/analysing-text-statistics.ipynb&depth=1\" target=\"_parent\"><img src=\"https://raw.githubusercontent.com/callysto/curriculum-notebooks/master/open-in-callysto-button.svg?sanitize=true\" width=\"123\" height=\"24\" alt=\"Open in Callysto\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing Text Statistics\n",
    "\n",
    "Let's try out some statistical analysis of text, including [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing), using a [public domain](https://en.wikipedia.org/wiki/Public_domain) book from [Project Gutenberg](http://www.gutenberg.org).\n",
    "\n",
    "The example we'll use is the [most downloaded](http://www.gutenberg.org/browse/scores/top) book, [Pride and Prejudice by Jane Austen](http://www.gutenberg.org/ebooks/1342). Running this first code cell will import and display the contents of the book.\n",
    "\n",
    "Feel free to change the link in the following code cell if you'd like to explore another book, but make sure you are using the `Plain Text UTF-8` link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_text_link = 'http://gutenberg.org/files/1342/1342-0.txt'\n",
    "\n",
    "import requests\n",
    "r = requests.get(gutenberg_text_link) # get the online book file\n",
    "r.encoding = 'utf-8' # specify the type of text encoding in the file\n",
    "text = r.text.split('***')[2] # get the part after the header\n",
    "text = text.replace(\"’\",\"'\").replace(\"“\",'\"').replace(\"”\",'\"') # replace any 'smart quotes'\n",
    "book_title = r.text[r.text.index('Title:')+7:r.text.index('Author:')-4] # find the book title\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a DataFrame of Chapters\n",
    "\n",
    "Now that we have the text of the book, let's split it into chapters. We'll use the Python [library](https://en.wikipedia.org/wiki/Library_(computing)) called [pandas](https://pandas.pydata.org) to create a [dataframe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) that includes the text and length of each chapter (number of characters, including spaces and punctuation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "chapters = pd.DataFrame() # create an empty data frame that we will append to\n",
    "for chapter in text.split('Chapter'):\n",
    "    if len(chapter)>500: # so that we are getting actual book chapters\n",
    "        chapter = chapter.replace('\\r','').replace('\\n','') # delete the 'new line' characters\n",
    "        chapters = chapters.append({'Chapter Text':chapter, 'Length':len(chapter)}, ignore_index=True)\n",
    "chapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Chapter Lengths\n",
    "\n",
    "From that dataframe we can create a bar graph of chapter lengths using the [cufflinks](https://github.com/santosjorge/cufflinks) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "chapters.iplot(kind='bar', y='Length', \n",
    "               title='Chapter Lengths in '+book_title, \n",
    "               yTitle='Length (characters)', xTitle='Chapter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Words by Type\n",
    "\n",
    "We'll use the [spaCy](https://spacy.io) natural language processing library to identify the [parts of speech](https://spacy.io/api/annotation#pos-tagging) in the text. For this example we'll just look at adjectives, verbs, nouns, and proper nouns, but you can add to the list on the first line in the code cell.\n",
    "\n",
    "This will take a while to run, and will result in a dataframe containing the number of each of those parts of speech in each chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_types = ['ADJ', 'VERB', 'NOUN', 'PROPN'] # https://spacy.io/api/annotation#pos-tagging\n",
    "\n",
    "# uncomment the following two installation lines if you get errors\n",
    "#!pip install spacy --user\n",
    "#!python -m spacy download en_core_web_sm\n",
    "\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load() # set up natural language processing\n",
    "parts_of_speech_df = pd.DataFrame(columns=word_types) # create an empty dataframe with column names\n",
    "for i in range(len(chapters)): # iterate through the chapters dataframe\n",
    "    parts_of_speech_list = [] # create an empty list\n",
    "    for token in nlp(chapters['Chapter Text'][i]): # loop through each token in the chapter\n",
    "        part_of_speech = token.pos_\n",
    "        if part_of_speech in word_types: # if it is in the list of types we made\n",
    "            parts_of_speech_list.append(part_of_speech)\n",
    "    word_type_count = {} # create an empty dictionary\n",
    "    for word_type in word_types:\n",
    "        word_type_count.update({word_type:parts_of_speech_list.count(word_type)}) # add to that dictionary\n",
    "    parts_of_speech_df = parts_of_speech_df.append(word_type_count, ignore_index=True) # add to dataframe\n",
    "parts_of_speech_df['Chapter'] = parts_of_speech_df.index+1 # create a Chapter column\n",
    "parts_of_speech_df = parts_of_speech_df.set_index('Chapter') # set the dataframe index to be that new column\n",
    "parts_of_speech_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the occurences of those parts of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_of_speech_df.iplot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Common Verbs\n",
    "\n",
    "To get an idea of the most common words in the text we can look at a part of speech, verbs for example, and count which are the most frequent.\n",
    "\n",
    "This will also take some time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_type = 'VERB'\n",
    "\n",
    "from collections import Counter\n",
    "words_df = pd.DataFrame()\n",
    "for i in range(len(chapters)): # loop through the chapters dataframe\n",
    "    word_list = []\n",
    "    for token in nlp(chapters['Chapter Text'][i]):\n",
    "        if token.pos_ == word_type: # if the token is a VERB\n",
    "            word_list.append(token.lemma_.strip().lower())\n",
    "    words_df = words_df.append(Counter(word_list), ignore_index=True) # add the list to the dataframe\n",
    "words_df.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example text there are 1233 unique verbs. Let's look at the `10` most common verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also choose a verb and plot its frequency by chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'say'\n",
    "\n",
    "words_df.iplot(y=word, title='Frequency of the Word \"'+word+'\" by Chapter', yTitle='Frequency', xTitle='Chapter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Common Names\n",
    "\n",
    "We can also look at character names and how often they occur in each chapter. The spaCy library does a fairly good job of identifying names, but you may see some false positives (words that aren't actually character names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_df = pd.DataFrame()\n",
    "for i in range(len(chapters)):\n",
    "    names_list = []\n",
    "    for token in nlp(chapters['Chapter Text'][i]):\n",
    "        #if token.pos_ == 'PROPN':\n",
    "        if token.ent_type_ == 'PERSON': # seems to be more reliable than part_of_speech == proper_noun\n",
    "            names_list.append(token.text)\n",
    "    names_df = names_df.append(Counter(names_list), ignore_index=True)\n",
    "names_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of Character Names\n",
    "\n",
    "We can check out the list of words identified as names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in names_df.columns:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data\n",
    "\n",
    "If you'd like to remove columns that may categorized incorrectly or are uncommon, we can drop columns with fewer than five occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in names_df.columns:\n",
    "    if names_df[column].sum() < 5: # if there are fewer than five occurences\n",
    "        names_df.drop(columns=column, inplace=True)\n",
    "names_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Name Frequencies\n",
    "\n",
    "Let's make a bar graph of the top `20` most frequently mentioned characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_df.sum().sort_values(ascending=False).head(20).iplot(kind='bar', yTitle='Frequency', title='Character Name Frequencies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name Frequencies over Time\n",
    "\n",
    "Since we have the text divided into chapters, let's visualize how often the top `3` character names are mentioned per chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_character_names = names_df.sum().sort_values(ascending=False).head(3).index # list the top three\n",
    "main_characters = names_df[main_character_names].fillna(value=0) # create a new dataframe for top three\n",
    "main_characters.iplot(yTitle='Frequency', xTitle='Chapter', title='Frequency of Character Mentions by Chapter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative Name Frequencies over Time\n",
    "\n",
    "We can also see the total number of times a character's name has been mentioned throughout the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_characters.cumsum().iplot(yTitle='Frequency', xTitle='Chapter', title='Cumulative Character Mentions')\n",
    "#names_df[names_df.sum().sort_values(ascending=False).head(5).index].fillna(value=0).cumsum().iplot(yTitle='Frequency', xTitle='Chapter', title='Cumulative Character Mentions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "[Sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) is categorizing text based on its tone (negative, neutral, or positive).\n",
    "\n",
    "For this we will use the [vaderSentiment](https://github.com/cjhutto/vaderSentiment) library, then visualize the positive and negative sentiment by chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install vaderSentiment --user\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "sentiment_df = pd.DataFrame()\n",
    "for i in range(len(chapters)):\n",
    "    senti = analyzer.polarity_scores(chapters['Chapter Text'][i]) # analyze the sentiment of chapter\n",
    "    sentiment_df = sentiment_df.append(senti, ignore_index=True) # add to dataframe\n",
    "sentiment_df['neg'] = -sentiment_df['neg'] # change the sign of the negative sentiment\n",
    "sentiment_df.iplot(y=['pos', 'neg'], kind='bar', title='Sentiment Analysis by Chapter', xTitle='Chapter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readability\n",
    "\n",
    "One last library to introduce, [textstat](https://github.com/shivam5992/textstat) for checking the readability, complexity, and grade level of text. It includes a [number of functions](https://github.com/shivam5992/textstat#list-of-functions), but we'll only use a few of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --user textstat\n",
    "import textstat\n",
    "readability = pd.DataFrame()\n",
    "for i in range(len(chapters)):\n",
    "    text = chapters['Chapter Text'][i]\n",
    "    readability_data = {'Flesch-Kincaid Grade':textstat.flesch_kincaid_grade(text),\n",
    "                        'Gunning Fog Index':textstat.gunning_fog(text),\n",
    "                        'Linsear Write Formula':textstat.linsear_write_formula(text),\n",
    "                        'Readability':textstat.text_standard(text, float_output=True)}\n",
    "    readability = readability.append(readability_data, ignore_index=True)\n",
    "readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a dataframe of readability information, we can plot and describe the readability statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readability.iplot(yTitle='Level', xTitle='Chapter', title='Text Statistics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readability.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Hopefully this was an interesting introduction to text statistics such as chapter length, word frequency and word type frequency, sentiment analysis, and reading levels. You can, of course, use this analyse the text from any other online document in a similar way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Callysto.ca License](https://github.com/callysto/curriculum-notebooks/blob/master/callysto-notebook-banner-bottom.jpg?raw=true)](https://github.com/callysto/curriculum-notebooks/blob/master/LICENSE.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
